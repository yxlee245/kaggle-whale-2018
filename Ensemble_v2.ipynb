{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble averages consisting of simple CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from src.sampler import up_down_sampler\n",
    "from src.model_api import getSimpleModel\n",
    "from src.prediction import array_filter, array_to_string\n",
    "from src.custom_metric import as_keras_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_train: (15697, 128, 128, 3)\n",
      "Length of labels_train: 15697\n",
      "Shape of data_test: (7960, 128, 128, 3)\n",
      "Length of filename_test: 7960\n"
     ]
    }
   ],
   "source": [
    "with open('data/derived/data_train_v3.pickle', 'rb') as file:\n",
    "    data_train = pickle.load(file)\n",
    "with open('data/derived/labels_train_v3.pickle', 'rb') as file:\n",
    "    labels_train = pickle.load(file)\n",
    "with open('data/derived/data_test_v2.pickle', 'rb') as file:\n",
    "    data_test = pickle.load(file)\n",
    "with open('data/derived/test_file_names.pickle', 'rb') as file:\n",
    "    filenames_test = pickle.load(file)\n",
    "print('Shape of data_train:', data_train.shape)\n",
    "print('Length of labels_train:', len(labels_train))\n",
    "print('Shape of data_test:', data_test.shape)\n",
    "print('Length of filename_test:', len(filenames_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert labels to integers for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels: ['w_f48451c', 'w_c3d896a', 'w_20df2c5', 'w_dd88965', 'w_64404ac']\n",
      "Encoded labels: [4785 3807  661 4314 1928]\n",
      "Label encoder classes: ['w_0003639' 'w_0003c59' 'w_0027efa' 'w_00289b1' 'w_002c810']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels_train_encoded = label_encoder.fit_transform(labels_train)\n",
    "print('Original labels: {}'.format(labels_train[:5]))\n",
    "print('Encoded labels: {}'.format(labels_train_encoded[:5]))\n",
    "print('Label encoder classes: {}'.format(label_encoder.classes_[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute sample size and number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 15697\n",
      "Number of clases: 5004\n"
     ]
    }
   ],
   "source": [
    "sample_size = len(labels_train_encoded)\n",
    "num_classes = len(set(labels_train_encoded))\n",
    "print('Sample size:', sample_size)\n",
    "print('Number of clases:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data_train\n",
    "data_train_norm = (data_train / 255).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'models/weights-{epoch:02d}-{val_loss:.3f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, save_weights_only=True,\n",
    "                             mode='min')\n",
    "early_stop = EarlyStopping(patience=2, monitor='val_loss', restore_best_weights=True)\n",
    "\n",
    "# Create precision and recall metrics\n",
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)\n",
    "metric_list = ['accuracy', precision, recall]\n",
    "\n",
    "model = getSimpleModel(num_classes=num_classes, resize_width=128, metric_list=metric_list)\n",
    "\n",
    "batch_size = 32\n",
    "image_gen = ImageDataGenerator(rotation_range=20,\n",
    "                               width_shift_range=.1,\n",
    "                               height_shift_range=.1,\n",
    "                               shear_range=0.5,\n",
    "                               zoom_range=(0.9, 1.1),\n",
    "                               fill_mode='constant',\n",
    "                               horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train n base models (random seeds 1 to n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c962cab59a50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                      \u001b[0msize_per_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                      random_state=model_no)\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_train_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_train_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidation_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_train_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_no_total = 3\n",
    "for model_no in range(1, model_no_total + 1):\n",
    "    sample_index, validation_index = up_down_sampler(labels_array=labels_train_encoded,\n",
    "                                                     validation_size=1000,\n",
    "                                                     size_per_class=20,\n",
    "                                                     random_state=model_no)\n",
    "    X_train = data_train_norm[sample_index]\n",
    "    X_test = data_train_norm[validation_index]\n",
    "    y_train = labels_train_encoded[sample_index]\n",
    "    y_test = labels_train_encoded[validation_index]\n",
    "    \n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "    image_generator_samples = image_gen.flow(X_train, y_train, batch_size=batch_size, seed=model_no)\n",
    "\n",
    "    np.random.seed(model_no)\n",
    "    set_random_seed(model_no)\n",
    "    train_size, epochs = X_train.shape[0], 10\n",
    "    model.fit_generator(image_generator_samples, steps_per_epoch=5*train_size//epochs, \n",
    "                        validation_data=(X_test, y_test), epochs=epochs, shuffle=True,\n",
    "                        callbacks=[checkpoint, early_stop])\n",
    "\n",
    "    model.save_weights('models/weights_simple_{}.hdf5'.format(model_no))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model weights and compute ensemble average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getSimpleModel(num_classes=num_classes, resize_width=128, metric_list=metric_list)\n",
    "data_test_norm = (data_test / 255).astype(np.float32)\n",
    "model_no_total = 3\n",
    "average_preds = np.zeros((len(filenames_test, num_classes)))\n",
    "for model_no in range(1, model_no_total + 1):\n",
    "    model.load_weights('models/weights_simple_{}.hdf5'.format(model_no))\n",
    "    pred = model.predict(data_test_norm)\n",
    "    average_preds += pred / model_no_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain labels with top 5 softmax values for each array row and concatenate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_indices = np.apply_along_axis(array_filter, arr=average_preds, axis=1, n_top=5,\n",
    "                                   labels=label_encoder.classes_, threshold=0.02)\n",
    "predictions_array = np.apply_along_axis(array_to_string, arr=top5_indices, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create submission DataFrame and export as CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Image                                                 Id\n",
      "0  c303faac6.jpg  new_whale w_17b0d3a w_789c969 w_67a9841 w_a9304b9\n",
      "1  96c2b7290.jpg  new_whale w_af367c3 w_8c25681 w_6822dbc w_f765256\n",
      "2  69f6cd44f.jpg  new_whale w_23a388d w_d405854 w_5773c71 w_03670aa\n",
      "3  a965dea33.jpg  new_whale w_1f0cf0a w_3de579a w_985d205 w_cd4cb49\n",
      "4  9a225e056.jpg  new_whale w_5a2634c w_700ebb4 w_0a155b9 w_23a388d\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({'Image': filenames_test, 'Id': predictions_array})\n",
    "submission_df.to_csv('data/derived/submission_v6.csv', index=False)\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle score: 0.286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
