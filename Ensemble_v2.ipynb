{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble averages consisting of simple CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from src.sampler import undersampler\n",
    "from src.model_api import getSimpleModel\n",
    "from src.prediction import array_filter, array_to_string\n",
    "from src.custom_metric import as_keras_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_train: (15697, 128, 128, 3)\n",
      "Length of labels_train: 15697\n",
      "Shape of data_test: (7960, 128, 128, 3)\n",
      "Length of filename_test: 7960\n"
     ]
    }
   ],
   "source": [
    "with open('data/derived/data_train_v3.pickle', 'rb') as file:\n",
    "    data_train = pickle.load(file)\n",
    "with open('data/derived/labels_train_v3.pickle', 'rb') as file:\n",
    "    labels_train = pickle.load(file)\n",
    "with open('data/derived/data_test_v2.pickle', 'rb') as file:\n",
    "    data_test = pickle.load(file)\n",
    "with open('data/derived/test_file_names.pickle', 'rb') as file:\n",
    "    filenames_test = pickle.load(file)\n",
    "print('Shape of data_train:', data_train.shape)\n",
    "print('Length of labels_train:', len(labels_train))\n",
    "print('Shape of data_test:', data_test.shape)\n",
    "print('Length of filename_test:', len(filenames_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert labels to integers for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels: ['w_f48451c', 'w_c3d896a', 'w_20df2c5', 'w_dd88965', 'w_64404ac']\n",
      "Encoded labels: [4785 3807  661 4314 1928]\n",
      "Label encoder classes: ['w_0003639' 'w_0003c59' 'w_0027efa' 'w_00289b1' 'w_002c810']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels_train_encoded = label_encoder.fit_transform(labels_train)\n",
    "print('Original labels: {}'.format(labels_train[:5]))\n",
    "print('Encoded labels: {}'.format(labels_train_encoded[:5]))\n",
    "print('Label encoder classes: {}'.format(label_encoder.classes_[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute sample size and number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 15697\n",
      "Number of clases: 5004\n"
     ]
    }
   ],
   "source": [
    "sample_size = len(labels_train_encoded)\n",
    "num_classes = len(set(labels_train_encoded))\n",
    "print('Sample size:', sample_size)\n",
    "print('Number of clases:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data_train\n",
    "data_train_norm = data_train / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'models/weights-{epoch:02d}-{val_loss:.3f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, save_weights_only=True,\n",
    "                             mode='min')\n",
    "early_stop = EarlyStopping(patience=2, monitor='val_loss', restore_best_weights=True)\n",
    "\n",
    "# Create precision and recall metrics\n",
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)\n",
    "metric_list = ['accuracy', precision, recall]\n",
    "\n",
    "model = getSimpleModel(num_classes=num_classes, resize_width=128, metric_list=metric_list)\n",
    "\n",
    "batch_size = 32\n",
    "image_gen = ImageDataGenerator(rotation_range=20,\n",
    "                               width_shift_range=.1,\n",
    "                               height_shift_range=.1,\n",
    "                               shear_range=0.5,\n",
    "                               zoom_range=(0.9, 1.1),\n",
    "                               fill_mode='constant',\n",
    "                               horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train n base models (random seeds 1 to n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2502/2502 [==============================] - 206s 82ms/step - loss: 8.5255 - acc: 1.2490e-05 - precision: 1.9976e-04 - recall: 0.9996 - val_loss: 8.5183 - val_acc: 2.0000e-04 - val_precision: 1.9984e-04 - val_recall: 1.0000\n",
      "Epoch 2/10\n",
      "2502/2502 [==============================] - 203s 81ms/step - loss: 8.5239 - acc: 1.2490e-04 - precision: 1.9983e-04 - recall: 1.0000 - val_loss: 8.5181 - val_acc: 2.0000e-04 - val_precision: 1.9983e-04 - val_recall: 1.0000\n",
      "Epoch 3/10\n",
      "2502/2502 [==============================] - 203s 81ms/step - loss: 8.5237 - acc: 8.7430e-05 - precision: 1.9982e-04 - recall: 1.0000 - val_loss: 8.5181 - val_acc: 2.0000e-04 - val_precision: 1.9982e-04 - val_recall: 1.0000\n",
      "Epoch 4/10\n",
      " 395/2502 [===>..........................] - ETA: 2:43 - loss: 8.5237 - acc: 0.0000e+00 - precision: 1.9982e-04 - recall: 1.0000"
     ]
    }
   ],
   "source": [
    "model_no_total = 3\n",
    "for model_no in range(1, model_no_total + 1):\n",
    "    sample_index, validation_index = undersampler(labels_array=labels_train_encoded, validation_size=5000,\n",
    "                                                  random_state=model_no)\n",
    "    X_train = data_train_norm[sample_index]\n",
    "    X_test = data_train_norm[validation_index]\n",
    "    y_train = labels_train_encoded[sample_index]\n",
    "    y_test = labels_train_encoded[validation_index]\n",
    "    \n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "    image_generator_samples = image_gen.flow(X_train, y_train, batch_size=batch_size, seed=model_no)\n",
    "\n",
    "    np.random.seed(model_no)\n",
    "    set_random_seed(model_no)\n",
    "    train_size, epochs = X_train.shape[0], 10\n",
    "    model.fit_generator(image_generator_samples, steps_per_epoch=5*train_size//epochs, \n",
    "                        validation_data=(X_test, y_test), epochs=epochs, shuffle=True,\n",
    "                        callbacks=[checkpoint, early_stop])\n",
    "\n",
    "    model.save_weights('models/weights_simple_{}.hdf5'.format(model_no))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_model_1.load_weights('models/weights_resnet50_1.hdf5')\n",
    "resnet50_model_2.load_weights('models/weights_resnet50_2.hdf5')\n",
    "resnet50_model_3.load_weights('models/weights_resnet50_3.hdf5')\n",
    "resnet50_model_4.load_weights('models/weights_resnet50_4.hdf5')\n",
    "densenet201_model_1.load_weights('models/weights_densenet201_1.hdf5')\n",
    "densenet201_model_2.load_weights('models/weights_densenet201_2.hdf5')\n",
    "densenet201_model_3.load_weights('models/weights_densenet201_3.hdf5')\n",
    "densenet201_model_4.load_weights('models/weights_densenet201_4.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble v1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All model results having same weightage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_preds_1 = resnet50_model_1.predict(resnet50_preprocess(data_test))\n",
    "resnet50_preds_2 = resnet50_model_2.predict(resnet50_preprocess(data_test))\n",
    "resnet50_preds_3 = resnet50_model_3.predict(resnet50_preprocess(data_test))\n",
    "resnet50_preds_4 = resnet50_model_4.predict(resnet50_preprocess(data_test))\n",
    "densenet201_preds_1 = densenet201_model_1.predict(densenet201_preprocess(data_test))\n",
    "densenet201_preds_2 = densenet201_model_2.predict(densenet201_preprocess(data_test))\n",
    "densenet201_preds_3 = densenet201_model_3.predict(densenet201_preprocess(data_test))\n",
    "densenet201_preds_4 = densenet201_model_4.predict(densenet201_preprocess(data_test))\n",
    "overall_preds = resnet50_preds_1 + resnet50_preds_2 + resnet50_preds_3 + resnet50_preds_4 +\\\n",
    "densenet201_preds_1 + densenet201_preds_2 + densenet201_preds_3 + densenet201_preds_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain labels with top 5 softmax values for each array row and concatenate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_indices = np.apply_along_axis(array_filter, arr=overall_preds, axis=1, n_top=5, labels=label_encoder.classes_)\n",
    "predictions_array = np.apply_along_axis(array_to_string, arr=top5_indices, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create submission DataFrame and export as CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Image                                                 Id\n",
      "0  c303faac6.jpg  new_whale w_17b0d3a w_789c969 w_67a9841 w_a9304b9\n",
      "1  96c2b7290.jpg  new_whale w_af367c3 w_8c25681 w_6822dbc w_f765256\n",
      "2  69f6cd44f.jpg  new_whale w_23a388d w_d405854 w_5773c71 w_03670aa\n",
      "3  a965dea33.jpg  new_whale w_1f0cf0a w_3de579a w_985d205 w_cd4cb49\n",
      "4  9a225e056.jpg  new_whale w_5a2634c w_700ebb4 w_0a155b9 w_23a388d\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({'Image': filenames_test, 'Id': predictions_array})\n",
    "submission_df.to_csv('submission_v1.csv', index=False)\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle score: 0.286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble v1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models weighted by accuracy score during training and validation phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_preds_1 = resnet50_model_1.predict(resnet50_preprocess(data_test))\n",
    "resnet50_preds_2 = resnet50_model_2.predict(resnet50_preprocess(data_test))\n",
    "resnet50_preds_3 = resnet50_model_3.predict(resnet50_preprocess(data_test))\n",
    "resnet50_preds_4 = resnet50_model_4.predict(resnet50_preprocess(data_test))\n",
    "densenet201_preds_1 = densenet201_model_1.predict(densenet201_preprocess(data_test))\n",
    "densenet201_preds_2 = densenet201_model_2.predict(densenet201_preprocess(data_test))\n",
    "densenet201_preds_3 = densenet201_model_3.predict(densenet201_preprocess(data_test))\n",
    "densenet201_preds_4 = densenet201_model_4.predict(densenet201_preprocess(data_test))\n",
    "overall_preds = 0.38*resnet50_preds_1 + 0.38*resnet50_preds_2 + 0.58*resnet50_preds_3 + 0.57*resnet50_preds_4 +\\\n",
    "0.38*densenet201_preds_1 + 0.38*densenet201_preds_2 + 0.58*densenet201_preds_3 + 0.57*densenet201_preds_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain labels with top 5 softmax values for each array row and concatenate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_indices = np.apply_along_axis(array_filter, arr=overall_preds, axis=1, n_top=5, labels=label_encoder.classes_)\n",
    "predictions_array = np.apply_along_axis(array_to_string, arr=top5_indices, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create submission DataFrame and export as CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Image                                                 Id\n",
      "0  c303faac6.jpg  new_whale w_17b0d3a w_67a9841 w_a9304b9 w_789c969\n",
      "1  96c2b7290.jpg  new_whale w_af367c3 w_8c25681 w_6822dbc w_f765256\n",
      "2  69f6cd44f.jpg  new_whale w_d405854 w_23a388d w_5773c71 w_fd3e556\n",
      "3  a965dea33.jpg  new_whale w_1f0cf0a w_cd4cb49 w_3de579a w_343f088\n",
      "4  9a225e056.jpg  new_whale w_5a2634c w_700ebb4 w_0a155b9 w_17b0d3a\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({'Image': filenames_test, 'Id': predictions_array})\n",
    "submission_df.to_csv('submission_v2.csv', index=False)\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle score: 0.286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
